{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn2_number_with_blank.ipynb","provenance":[{"file_id":"1KmnJv1WLStVh5vB-YOBsZoLLqHZjsrvk","timestamp":1580360697728}],"collapsed_sections":[],"authorship_tag":"ABX9TyM+GF9PTQvJCRi1EojdlcQS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZaT1LNGnKI4Q","colab_type":"code","outputId":"441dc5e6-79c8-42f7-de76-8219108d9905","executionInfo":{"status":"ok","timestamp":1580366304330,"user_tz":-540,"elapsed":1367,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# stack, concatenate, reshape, indexing\n","# stack(배열들을 하나로) 쌓는다. 차원이 1 증가한다.\n","a = np.arange(12).reshape(3, 4)\n","b = a\n","c = np.stack((a, b, a, b), axis=2)\n","c.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 4, 4)"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"88gNlZ-oLP3W","colab_type":"code","outputId":"b6bd6398-a05a-4351-9a42-de986f283d89","executionInfo":{"status":"ok","timestamp":1580366435363,"user_tz":-540,"elapsed":850,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# concatenate(잇다 붙이다)\n","# 차원이 증가하지 않는다.\n","a = np.arange(12).reshape(3, 4)\n","b = a\n","c = np.concatenate((a, b), axis=0)\n","c.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6, 4)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"hJyjf5FLKa9X","colab_type":"code","outputId":"9fc940ec-40c9-466d-c855-08fed6b0efe1","executionInfo":{"status":"ok","timestamp":1580366438922,"user_tz":-540,"elapsed":775,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["c"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  1,  2,  3],\n","       [ 4,  5,  6,  7],\n","       [ 8,  9, 10, 11],\n","       [ 0,  1,  2,  3],\n","       [ 4,  5,  6,  7],\n","       [ 8,  9, 10, 11]])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"y_CTYzpfL2im","colab_type":"code","outputId":"d36cd69e-532d-4477-d2aa-8fa2ae31a5a7","executionInfo":{"status":"ok","timestamp":1580366555455,"user_tz":-540,"elapsed":827,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# reshape\n","c.reshape(3, 4, -1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 0,  1],\n","        [ 2,  3],\n","        [ 4,  5],\n","        [ 6,  7]],\n","\n","       [[ 8,  9],\n","        [10, 11],\n","        [ 0,  1],\n","        [ 2,  3]],\n","\n","       [[ 4,  5],\n","        [ 6,  7],\n","        [ 8,  9],\n","        [10, 11]]])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"5uhcax6AMHd3","colab_type":"code","outputId":"71874a37-13d8-47e2-d85f-25ab14ca7868","executionInfo":{"status":"ok","timestamp":1580366617727,"user_tz":-540,"elapsed":858,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# indexing\n","# 데이터 뽑기\n","a = np.arange(12).reshape(3, 4)\n","a"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0,  1,  2,  3],\n","       [ 4,  5,  6,  7],\n","       [ 8,  9, 10, 11]])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"7XVTkvjwMOOO","colab_type":"code","outputId":"75b54fba-f894-4972-d1d4-86b341d1e722","executionInfo":{"status":"ok","timestamp":1580366667830,"user_tz":-540,"elapsed":962,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["a[-1, 2]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"uWgdmwZuMlWG","colab_type":"code","outputId":"91fd15fc-b7f4-42f6-b597-c260cfa5226e","executionInfo":{"status":"ok","timestamp":1580366736914,"user_tz":-540,"elapsed":819,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# slicing\n","a[0:2, 1]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 5])"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"T685EpcI2M1F","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_digits\n","import random\n","from skimage.transform import rotate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdetNQev2ez5","colab_type":"code","outputId":"9da51fb8-0d16-4b96-d9a5-8abad4e4ca9c","executionInfo":{"status":"ok","timestamp":1581053845240,"user_tz":-540,"elapsed":825,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x, y = load_digits(return_X_y=True)\n","x = np.reshape(x, [-1, 8, 8])\n","x.shape, y.shape, set(y)"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1797, 8, 8), (1797,), {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"CDtgw3XRJzD0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"38074129-cb3b-4c34-e087-f9dda1a6c20d","executionInfo":{"status":"ok","timestamp":1581053882415,"user_tz":-540,"elapsed":1040,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["plt.title(y[102])\n","plt.imshow(x[102])"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa973256518>"]},"metadata":{"tags":[]},"execution_count":48},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPUAAAEICAYAAACHyrIWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\negAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"1pxvaq8x1-Wl","colab_type":"code","colab":{}},"source":["m = len(y) // 2 # 훈련, 테스트 데이터 반으로 나누기 위해 사용하는 숫자\n","x_train = x[:m]\n","y_train = y[:m]\n","x_test = x[m:m*2]\n","y_test = y[m:m*2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4JVfZjuA3KF","colab_type":"code","outputId":"6d465afa-9c0d-46e6-abfe-006449c1fe9b","executionInfo":{"status":"ok","timestamp":1581053984616,"user_tz":-540,"elapsed":622,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x_train.shape, y_train.shape, x_test.shape, y_test.shape"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((898, 8, 8), (898,), (898, 8, 8), (898,))"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"ec-yzM-XHpf0","colab_type":"text"},"source":["train data 증식하기 - 각도 조절"]},{"cell_type":"code","metadata":{"id":"NDo_EZdiHpE_","colab_type":"code","colab":{}},"source":["x_train_angle = []\n","y_train_angle = []\n","\n","for idx, image in enumerate(x_train):\n","  angle = random.randrange(-20, 20)\n","  img = np.array(image)\n","  img = rotate(img, angle)\n","  x_train_angle.append(np.round(img))\n","  y_train_angle.append(y_train[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnHdoHjtU0P8","colab_type":"code","outputId":"88ba09b7-09e9-40a4-83a8-9223a6b849ba","executionInfo":{"status":"ok","timestamp":1581054074706,"user_tz":-540,"elapsed":889,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x_train = np.vstack((x_train, x_train_angle))\n","y_train = np.concatenate((y_train, y_train_angle))\n","y_train = np.reshape(y_train, [-1, 1])\n","x_train.shape, y_train.shape"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1796, 8, 8), (1796, 1))"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"G0xCxf8MTElA","colab_type":"code","outputId":"3a6b881b-ff81-4b59-a7dc-79f210a8dc2d","executionInfo":{"status":"ok","timestamp":1581054264939,"user_tz":-540,"elapsed":873,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["BLANK = 10\n","x_train_blank = np.concatenate((x_train, np.zeros_like(x_train)), axis=2)\n","y_train_blank = np.concatenate((y_train, np.zeros_like(y_train)+BLANK), axis=1)\n","x_train_blank.shape, y_train_blank.shape"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1796, 8, 16), (1796, 2))"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"OP5RDhnMLZYe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"outputId":"6113edc1-9a13-400d-8c34-82951e2d6ea3","executionInfo":{"status":"ok","timestamp":1581054282298,"user_tz":-540,"elapsed":863,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["plt.imshow(x_train_blank[1])"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa97316a240>"]},"metadata":{"tags":[]},"execution_count":64},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWoAAADGCAYAAAD7ccrCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4/4XTlQm88P1fZEAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"kQqgSJSAG9U-","colab_type":"code","colab":{}},"source":["x_train_lr = []\n","y_train_lr = []\n","\n","for _ in range(len(y_train_blank)):\n","  rnd1 = random.randrange(0, len(x)-1)\n","  rnd2 = random.randrange(0, len(x)-1)\n","\n","  x_train_lr.append(np.reshape(np.stack((x[rnd1], x[rnd2]), axis=1), [8, 16]))\n","  y_train_lr.append([y[rnd1], y[rnd2]])\n","\n","x_train_lr = np.array(x_train_lr)\n","y_train_lr = np.array(y_train_lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EzX3uINUIngO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"ba729cba-03ac-45dc-8cca-75a4b6d8856a","executionInfo":{"status":"ok","timestamp":1581054428780,"user_tz":-540,"elapsed":843,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["x_train_lr.shape, y_train_lr.shape"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1796, 8, 16), (1796, 2))"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"R8G3Mt6KMA9j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":285},"outputId":"2166b9ec-a7fd-4129-d38e-62471ba9b2b2","executionInfo":{"status":"ok","timestamp":1581054453977,"user_tz":-540,"elapsed":825,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["plt.title(y_train_lr[0])\n","plt.imshow(x_train_lr[0])"],"execution_count":71,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  if s != self._text:\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa973160630>"]},"metadata":{"tags":[]},"execution_count":71},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWoAAADWCAYAAAD4p8hZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"HDXzgECfIsR-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4046dbd6-4b69-496b-c984-d7516f886dc9","executionInfo":{"status":"ok","timestamp":1581054469957,"user_tz":-540,"elapsed":860,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["x_train_set = np.concatenate((x_train_blank, x_train_lr), axis=0)\n","y_train_set = np.concatenate((y_train_blank, y_train_lr), axis=0)\n","x_train_set.shape, y_train_set.shape"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3592, 8, 16), (3592, 2))"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"LQkSBJMBdGTO","colab_type":"code","colab":{}},"source":["# x_train_lr = []\n","# y_train_lr = []\n","# for idx in range(len(y_train)//2):\n","#   x_train_lr.append(np.concatenate((x_train[idx*2], x_train[idx*2+1]), axis=1))\n","#   y_train_lr.append(np.concatenate((y_train[idx*2], y_train[idx*2+1])))\n","#   x_train_lr.append(np.concatenate((x_train[idx*2+1], x_train[idx*2]), axis=1))\n","#   y_train_lr.append(np.concatenate((y_train[idx*2+1], y_train[idx*2])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5O7-RYM5hdnC","colab_type":"code","outputId":"12535e79-5d52-4391-8499-dfbf528fb859","executionInfo":{"status":"ok","timestamp":1580396542138,"user_tz":-540,"elapsed":1691,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# x_train_lr = np.array(x_train_lr)\n","# y_train_lr = np.array(y_train_lr)\n","# x_train_lr.shape, y_train_lr.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1796, 8, 16), (1796, 2))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"CgNWpwGshrKG","colab_type":"code","outputId":"fbe3917c-f82f-4849-df99-633834677df9","executionInfo":{"status":"ok","timestamp":1580396544816,"user_tz":-540,"elapsed":2265,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# x_train_set = np.vstack((x_train_blank, x_train_lr))\n","# y_train_set = np.vstack((y_train_blank, y_train_lr))\n","# x_train_set.shape, y_train_set.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3592, 8, 16), (3592, 2))"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"A6qBuBIXiLZv","colab_type":"code","outputId":"89435193-13f1-4283-e526-fe3e491cc73c","executionInfo":{"status":"ok","timestamp":1581054523257,"user_tz":-540,"elapsed":880,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":285}},"source":["# 확인용\n","plt.title(y_train_set[3003])\n","plt.imshow(x_train_set[3003])"],"execution_count":75,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  if s != self._text:\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa9730fae80>"]},"metadata":{"tags":[]},"execution_count":75},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWoAAADWCAYAAAD4p8hZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"eBhZtKOWi0JT","colab_type":"code","outputId":"cb3e1c55-4d21-4c3f-d97d-0c5918dab9f7","executionInfo":{"status":"ok","timestamp":1581054541772,"user_tz":-540,"elapsed":875,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["random_idx = [x for x in range(len(y_train_set))]\n","random.shuffle(random_idx)\n","random_idx[:10]"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2186, 253, 2115, 1003, 2615, 3286, 1214, 1181, 747, 1510]"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"2Uc6snjpj6D_","colab_type":"code","colab":{}},"source":["x_train_shuffle = []\n","y_train_shuffle = []\n","\n","for idx in random_idx:\n","  x_train_shuffle.append(x_train_set[idx])\n","  y_train_shuffle.append(y_train_set[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ERhcMCy2N70","colab_type":"text"},"source":["이미지를 2개씩 좌우로 붙여 합성 데이터를 만든다"]},{"cell_type":"code","metadata":{"id":"6dbvJCJ12Q1c","colab_type":"code","outputId":"56f84b78-90a9-465c-8d20-187c61b93acc","executionInfo":{"status":"ok","timestamp":1581054576374,"user_tz":-540,"elapsed":948,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x_test_l, x_test_r = np.split(x_test, 2, axis=0) \n","x_test_lr = np.concatenate((x_test_l, x_test_r), axis=2)\n","y_test_l, y_test_r = np.split(y_test, 2, axis=0) \n","y_test = np.stack((y_test_l, y_test_r), -1)\n","x_test_lr.shape, y_test.shape"],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((449, 8, 16), (449, 2))"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"id":"HDHgzI_F2u5s","colab_type":"code","outputId":"4afa58e8-32e4-4627-ad84-e248134c456a","executionInfo":{"status":"ok","timestamp":1581054577512,"user_tz":-540,"elapsed":751,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["BLANK = 10 #'없음' 을 나타내는 기호\n","x_test_blank = np.concatenate((x_test_l, np.zeros_like(x_test_r)), axis=2)\n","y_test_blank = np.stack((y_test_l, np.zeros_like(y_test_l) + BLANK), 1)#좌측 영상의 y와 빈 영상의 y를 통합\n","\n","x_test_set = np.concatenate((x_test_lr, x_test_blank), 0)# 두자리수 영상과 한자리수 영상을 통합\n","y_test_set = np.concatenate((y_test, y_test_blank), 0)#두자리수 y와 한자리수 y 를 통합\n","x_test_set.shape, y_test_set.shape"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((898, 8, 16), (898, 2))"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"id":"CrQPuI7N4GD9","colab_type":"code","outputId":"a3c1eef3-6c93-4783-8f88-2b1029162f63","executionInfo":{"status":"ok","timestamp":1581054579168,"user_tz":-540,"elapsed":1061,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":285}},"source":["plt.title(y_test_set[501])\n","plt.imshow(x_test_set[501])"],"execution_count":81,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/matplotlib/text.py:1150: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  if s != self._text:\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fa972f4eb70>"]},"metadata":{"tags":[]},"execution_count":81},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWoAAADWCAYAAAD4p8hZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1DiLWpIa9/94DNCpk3HCEQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hTAkUMUy17ia","colab":{}},"source":["# class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n","#   def __init__(self):    \n","#     super(MyModel, self).__init__()\n","#     self.k = 10+1 # 클래스 갯수 \n","#     self.seq = 2 # 자릿수\n","#     self.opt = tf.optimizers.Adam(learning_rate=0.0001,amsgrad=True)#Stochatic Gradient Descent 확률적 경사 하강\n","#     self.conv0 = keras.layers.Conv2D(64, [3,3], padding='same', activation=keras.activations.relu)\n","#     self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same', activation=keras.activations.relu)\n","#     self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n","#     self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')\n","#     self.flatten = keras.layers.Flatten()\n","#     self.dense0 = keras.layers.Dense(units=128)\n","#     self.dense1 = keras.layers.Dense(units=self.k * self.seq)\n","#     self.drop0 = keras.layers.Dropout(0.5)\n","#     self.drop1 = keras.layers.Dropout(0.4)\n","  \n","#   def call(self, x):\n","#     #x (1797, 64)\n","#     x_4d = tf.reshape(x, [-1,8,8*2,1]) \n","#     x_4d = tf.cast(x_4d, tf.float32)\n","#     net = self.conv0(x_4d)\n","#     net = self.pool0(net)\n","#     net = self.conv1(net)\n","#     net = self.pool1(net)\n","#     net = self.flatten(net)\n","#     net = self.drop0(net)    \n","#     net = self.dense0(net)\n","#     net = self.drop1(net)\n","#     h = self.dense1(net)\n","#     h = tf.reshape(h, [-1, self.seq, self.k]) # 2:두자리수, 10:10개의 클래스 \n","#     h = tf.nn.softmax(h, axis=2)\n","#     return h\n","\n","#   def get_loss(self, y, h):\n","#     #학습할때 nan이 발생하는 경우 값을 clip(자르다) (최소값, 최대값) \n","#     h = tf.clip_by_value(h, 1e-8, 1 - 1e-8) # h 가 0이나 1이 되지 않도록 하는 안전장치 \n","#     cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n","#     loss = tf.reduce_mean(cross_entropy)\n","#     return loss\n","\n","#   def get_accuracy(self, y, h):    \n","#     predict = tf.argmax(h, -1)\n","#     is_equal = tf.equal(y, predict)\n","#     self.acc = tf.reduce_mean(tf.cast(is_equal, tf.float32)) # True > 1, False > 0 로 cast\n","#     self.acc_all = tf.reduce_mean(tf.cast(tf.reduce_all(is_equal, axis=1), tf.float32))\n","\n","#   def fit(self, x, y, epoch=1):\n","#     # x : (m, 8, 16), y: (m, 2)    \n","#     y_hot = tf.one_hot(y, depth=self.k, axis=-1)#(m, 2, 10)  \n","#     for i in range(epoch):\n","#       with tf.GradientTape() as tape: #경사 기록 장치\n","#         h = self.call(x)\n","#         loss = self.get_loss(y_hot, h)        \n","#       grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n","#       grads = [(tf.clip_by_value(grad, -5.0, 5.5)) for grad in grads] \n","#       self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n","#       self.get_accuracy(y, h)\n","#       if i%10==0:\n","#         print('%d/%d loss:%.3f acc:%.3f acc_all:%.3f'%(i, epoch, loss, self.acc, self.acc_all))\n","# model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEPV3EUKMok-","colab_type":"code","colab":{}},"source":["class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n","  def __init__(self):    \n","    super(MyModel, self).__init__()\n","    self.k = 10+1 # 클래스 갯수 \n","    self.seq = 2 # 자릿수\n","    self.opt = tf.optimizers.Adam(learning_rate=0.0001,amsgrad=True)#Stochatic Gradient Descent 확률적 경사 하강\n","    self.conv0 = keras.layers.Conv2D(64, [3,3], padding='same', activation=keras.activations.relu)\n","    self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same', activation=keras.activations.relu)\n","    self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n","    self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')\n","    self.flatten = keras.layers.Flatten()\n","    self.dense0 = keras.layers.Dense(units=128)\n","    self.dense1 = keras.layers.Dense(units=self.k * self.seq)\n","    self.drop0 = keras.layers.Dropout(0.5)\n","    self.drop1 = keras.layers.Dropout(0.4)\n","  \n","  def call(self, x):\n","    #x (1797, 64)\n","    x_4d = tf.reshape(x, [-1,8,8*2,1]) \n","    x_4d = tf.cast(x_4d, tf.float32)\n","    net = self.conv0(x_4d)\n","    net = self.pool0(net)\n","    net = self.conv1(net)\n","    net = self.pool1(net)\n","    net = self.flatten(net)\n","    net = self.drop0(net)    \n","    net = self.dense0(net)\n","    net = self.drop1(net)\n","    h = self.dense1(net)\n","    h = tf.reshape(h, [-1, self.seq, self.k]) # 2:두자리수, 10:10개의 클래스 \n","    h = tf.nn.softmax(h, axis=2)\n","    return h\n","\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MWfdlobMxYF","colab_type":"code","colab":{}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","@tf.function\n","def train_step(images, labels):\n","  with tf.GradientTape() as tape:\n","    predictions = model(images)\n","    loss = loss_object(labels, predictions)\n","  gradients = tape.gradient(loss, model.trainable_variables) # loss에 대한 파라미터들의 도함수\n","  # list of (gradients, variables) pairs\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","  train_loss(loss)\n","  train_accuracy(labels, predictions)\n","\n","@tf.function\n","def test_step(images, labels):\n","  predictions = model(images)\n","  t_loss = loss_object(labels, predictions)\n","  \n","  test_loss(t_loss)\n","  test_accuracy(labels, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5v2ZWR5Mz8t","colab_type":"code","colab":{}},"source":["train_ds = tf.data.Dataset.from_tensor_slices((x_train_set, y_train_set)).shuffle(10000).batch(64)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test_set, y_test_set)).shuffle(10000).batch(64)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzT_u23qNIwc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":325},"outputId":"fe4cd0cf-a992-4938-bf1e-da107349b560","executionInfo":{"status":"ok","timestamp":1581056032729,"user_tz":-540,"elapsed":12918,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["EPOCHS = 100\n","\n","for epoch in range(EPOCHS):\n","\n","  for images, labels in train_ds:\n","    train_step(images, labels)\n","\n","  for test_images, test_labels in test_ds:\n","    test_step(test_images, test_labels)\n","\n","  template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n","\n","  if epoch % 10 == 0:\n","    print (template.format(epoch+1,\n","                         train_loss.result(),\n","                         train_accuracy.result()*100,\n","                         test_loss.result(),\n","                         test_accuracy.result()*100))"],"execution_count":101,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer my_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","에포크: 1, 손실: 1.072885513305664, 정확도: 67.27449798583984, 테스트 손실: 0.3046645224094391, 테스트 정확도: 92.26058197021484\n","에포크: 11, 손실: 0.13185551762580872, 정확도: 96.1264877319336, 테스트 손실: 0.06266213208436966, 테스트 정확도: 98.19801330566406\n","에포크: 21, 손실: 0.07024495303630829, 정확도: 97.95179748535156, 테스트 손실: 0.04109383746981621, 테스트 정확도: 98.78831481933594\n","에포크: 31, 손실: 0.04768639802932739, 정확도: 98.61250305175781, 테스트 손실: 0.032496362924575806, 테스트 정확도: 99.01214599609375\n","에포크: 41, 손실: 0.03609299287199974, 정확도: 98.95092010498047, 테스트 손실: 0.028006570413708687, 테스트 정확도: 99.12135314941406\n","에포크: 51, 손실: 0.029033521190285683, 정확도: 99.15662384033203, 테스트 손실: 0.02527422271668911, 테스트 정확도: 99.18773651123047\n","에포크: 61, 손실: 0.02428298629820347, 정확도: 99.29488372802734, 테스트 손실: 0.023337634280323982, 테스트 정확도: 99.22870635986328\n","에포크: 71, 손실: 0.020867805927991867, 정확도: 99.39419555664062, 테스트 손실: 0.0218734759837389, 테스트 정확도: 99.26676177978516\n","에포크: 81, 손실: 0.018294401466846466, 정확도: 99.46897888183594, 테스트 손실: 0.020774303004145622, 테스트 정확도: 99.29197692871094\n","에포크: 91, 손실: 0.016285808756947517, 정확도: 99.52733612060547, 테스트 손실: 0.019889477640390396, 테스트 정확도: 99.30921173095703\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Ff20UzbNjjP","colab_type":"code","outputId":"e5c579ce-81aa-45b9-8b0f-d79e088c01b4","executionInfo":{"status":"ok","timestamp":1580371343866,"user_tz":-540,"elapsed":927,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":["# 테스트셋의 성능\n","h = model(x_test_set)\n","model.get_accuracy(y_test_set, h)\n","model.acc, model.acc_all"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer my_model_30 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(), dtype=float32, numpy=0.95100224>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=0.9075724>)"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"mXsnOwtpe2O4","colab_type":"text"},"source":["RNN"]},{"cell_type":"code","metadata":{"id":"Y2TyE4Feer-4","colab_type":"code","colab":{}},"source":["class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n","  def __init__(self):    \n","    super(MyModel, self).__init__()\n","    self.k = 10+1 # 클래스 갯수 \n","    self.seq = 2 # 자릿수\n","    self.opt = tf.optimizers.Adam(learning_rate=0.001)#Stochatic Gradient Descent 확률적 경사 하강\n","    # self.conv0 = keras.layers.Conv2D(16, [3,3], padding='same', activation=keras.activations.relu)\n","    # self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same', activation=keras.activations.relu)\n","    self.conv0 = keras.layers.Conv2D(16, [3,3], padding='same')\n","    self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same')\n","    self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n","    self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')    \n","    # self.rnn = keras.layers.LSTM(units=self.k, return_sequences=True)\n","    self.rnn = keras.layers.GRU(units=self.k, return_sequences=True)\n","    self.batch = keras.layers.BatchNormalization()\n","    self.act = keras.layers.ReLU()\n","  \n","  def call(self, x):\n","    #x (1797, 64)\n","    x_4d = tf.reshape(x, [-1,8,8*2,1]) \n","    x_4d = tf.cast(x_4d, tf.float32)\n","    net = self.conv0(x_4d)\n","    # net = self.batch(net)\n","    net = self.act(net)\n","    net = self.pool0(net) #(4,8,16)\n","    net = self.conv1(net)\n","    # net = self.batch(net)\n","    net = self.act(net)\n","    net = self.pool1(net) #(none, 2, 4, 32)\n","    net = self.pool1(net) #(none, 1, 2, 32)\n","    net = tf.squeeze(net, axis=0) #(none, 2, 32)\n","    h = self.rnn(net)#(2,10)        \n","    h = tf.nn.softmax(h, axis=2)\n","    return h\n","\n","  # def get_loss(self, y, h):\n","  #   #학습할때 nan이 발생하는 경우 값을 clip(자르다) (최소값, 최대값) \n","  #   h = tf.clip_by_value(h, 1e-8, 1 - 1e-8) # h 가 0이나 1이 되지 않도록 하는 안전장치 \n","  #   cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n","  #   loss = tf.reduce_mean(cross_entropy)\n","  #   return loss\n","\n","  # def get_accuracy(self, y, h):    \n","  #   predict = tf.argmax(h, -1)\n","  #   is_equal = tf.equal(y, predict)\n","  #   self.acc = tf.reduce_mean(tf.cast(is_equal, tf.float32)) # True > 1, False > 0 로 cast\n","  #   self.acc_all = tf.reduce_mean(tf.cast(tf.reduce_all(is_equal, axis=1), tf.float32))\n","\n","  # def fit(self, x, y, epoch=1):\n","  #   # x : (m, 8, 16), y: (m, 2)    \n","  #   y_hot = tf.one_hot(y, depth=self.k, axis=-1)#(m, 2, 10)  \n","  #   for i in range(epoch):\n","  #     with tf.GradientTape() as tape: #경사 기록 장치\n","  #       h = self.call(x)\n","  #       loss = self.get_loss(y_hot, h)        \n","  #     grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n","  #     #경사가 너무 크면 nan 이 될 수 있으므로 gradient cliping (최소,최대값 제한) 을 합니다\n","  #     grads = [(tf.clip_by_value(grad, -5.0, 5.0)) for grad in grads]      \n","  #     self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n","  #     self.get_accuracy(y, h)\n","  #     if i%10==0:\n","  #       print('%d/%d loss:%.3f acc:%.3f acc_all:%.3f'%(i, epoch, loss, self.acc, self.acc_all))\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiBYCQ46TJHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"31a947db-030d-43ff-b8d3-c7878028ffd4","executionInfo":{"status":"error","timestamp":1581057147158,"user_tz":-540,"elapsed":888,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}}},"source":["EPOCHS = 100\n","\n","for epoch in range(EPOCHS):\n","\n","  for images, labels in train_ds:\n","    train_step(images, labels)\n","\n","  for test_images, test_labels in test_ds:\n","    test_step(test_images, test_labels)\n","\n","  template = '에포크: {}, 손실: {}, 정확도: {}, 테스트 손실: {}, 테스트 정확도: {}'\n","\n","  if epoch % 10 == 0:\n","    print (template.format(epoch+1,\n","                         train_loss.result(),\n","                         train_accuracy.result()*100,\n","                         test_loss.result(),\n","                         test_accuracy.result()*100))"],"execution_count":111,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer my_model_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-111-a41a5ae04032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 497\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-83-32aef599c88f>:13 train_step  *\n        predictions = model(images)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py:778 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    <ipython-input-109-f0b3f09c88a3>:31 call  *\n        net = tf.squeeze(net, axis=0) #(none, 2, 32)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/util/dispatch.py:180 wrapper\n        return target(*args, **kwargs)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/array_ops.py:3832 squeeze_v2\n        return squeeze(input, axis, name)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/util/dispatch.py:180 wrapper\n        return target(*args, **kwargs)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/util/deprecation.py:507 new_func\n        return func(*args, **kwargs)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/array_ops.py:3780 squeeze\n        return gen_array_ops.squeeze(input, axis, name)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_array_ops.py:9231 squeeze\n        \"Squeeze\", input=input, squeeze_dims=axis, name=name)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py:3322 _create_op_internal\n        op_def=op_def)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py:1786 __init__\n        control_input_ops)\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Can not squeeze dim[0], expected a dimension of 1, got 64 for 'my_model_7/Squeeze' (op: 'Squeeze') with input shapes: [64,1,2,32].\n"]}]},{"cell_type":"code","metadata":{"id":"cObQZUC6e703","colab_type":"code","outputId":"8b0549f9-2e2a-468a-f122-c8e3ba0b4405","executionInfo":{"status":"ok","timestamp":1580397497424,"user_tz":-540,"elapsed":39182,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit(x_train_set, y_train_set, 1500)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0/1500 loss:0.327 acc:0.079 acc_all:0.001\n","10/1500 loss:0.287 acc:0.285 acc_all:0.029\n","20/1500 loss:0.268 acc:0.367 acc_all:0.101\n","30/1500 loss:0.248 acc:0.627 acc_all:0.384\n","40/1500 loss:0.225 acc:0.754 acc_all:0.568\n","50/1500 loss:0.201 acc:0.836 acc_all:0.702\n","60/1500 loss:0.181 acc:0.891 acc_all:0.797\n","70/1500 loss:0.171 acc:0.923 acc_all:0.854\n","80/1500 loss:0.166 acc:0.938 acc_all:0.881\n","90/1500 loss:0.162 acc:0.949 acc_all:0.902\n","100/1500 loss:0.160 acc:0.960 acc_all:0.922\n","110/1500 loss:0.157 acc:0.965 acc_all:0.932\n","120/1500 loss:0.154 acc:0.968 acc_all:0.938\n","130/1500 loss:0.152 acc:0.960 acc_all:0.923\n","140/1500 loss:0.151 acc:0.965 acc_all:0.932\n","150/1500 loss:0.149 acc:0.968 acc_all:0.937\n","160/1500 loss:0.148 acc:0.970 acc_all:0.942\n","170/1500 loss:0.147 acc:0.971 acc_all:0.944\n","180/1500 loss:0.146 acc:0.974 acc_all:0.950\n","190/1500 loss:0.145 acc:0.976 acc_all:0.953\n","200/1500 loss:0.144 acc:0.978 acc_all:0.956\n","210/1500 loss:0.144 acc:0.979 acc_all:0.959\n","220/1500 loss:0.143 acc:0.980 acc_all:0.961\n","230/1500 loss:0.143 acc:0.983 acc_all:0.966\n","240/1500 loss:0.142 acc:0.985 acc_all:0.970\n","250/1500 loss:0.142 acc:0.986 acc_all:0.972\n","260/1500 loss:0.142 acc:0.987 acc_all:0.975\n","270/1500 loss:0.142 acc:0.988 acc_all:0.976\n","280/1500 loss:0.141 acc:0.988 acc_all:0.977\n","290/1500 loss:0.141 acc:0.989 acc_all:0.978\n","300/1500 loss:0.141 acc:0.989 acc_all:0.978\n","310/1500 loss:0.141 acc:0.990 acc_all:0.979\n","320/1500 loss:0.141 acc:0.990 acc_all:0.981\n","330/1500 loss:0.141 acc:0.991 acc_all:0.982\n","340/1500 loss:0.141 acc:0.991 acc_all:0.983\n","350/1500 loss:0.140 acc:0.992 acc_all:0.983\n","360/1500 loss:0.140 acc:0.992 acc_all:0.984\n","370/1500 loss:0.140 acc:0.992 acc_all:0.984\n","380/1500 loss:0.140 acc:0.992 acc_all:0.984\n","390/1500 loss:0.140 acc:0.992 acc_all:0.984\n","400/1500 loss:0.140 acc:0.992 acc_all:0.984\n","410/1500 loss:0.140 acc:0.992 acc_all:0.984\n","420/1500 loss:0.140 acc:0.992 acc_all:0.984\n","430/1500 loss:0.140 acc:0.992 acc_all:0.985\n","440/1500 loss:0.140 acc:0.992 acc_all:0.984\n","450/1500 loss:0.140 acc:0.992 acc_all:0.985\n","460/1500 loss:0.140 acc:0.992 acc_all:0.985\n","470/1500 loss:0.140 acc:0.993 acc_all:0.985\n","480/1500 loss:0.140 acc:0.993 acc_all:0.986\n","490/1500 loss:0.140 acc:0.993 acc_all:0.986\n","500/1500 loss:0.140 acc:0.993 acc_all:0.986\n","510/1500 loss:0.140 acc:0.993 acc_all:0.987\n","520/1500 loss:0.140 acc:0.994 acc_all:0.988\n","530/1500 loss:0.140 acc:0.994 acc_all:0.988\n","540/1500 loss:0.139 acc:0.994 acc_all:0.988\n","550/1500 loss:0.139 acc:0.994 acc_all:0.989\n","560/1500 loss:0.139 acc:0.994 acc_all:0.989\n","570/1500 loss:0.139 acc:0.994 acc_all:0.989\n","580/1500 loss:0.139 acc:0.994 acc_all:0.989\n","590/1500 loss:0.139 acc:0.994 acc_all:0.989\n","600/1500 loss:0.139 acc:0.994 acc_all:0.989\n","610/1500 loss:0.139 acc:0.994 acc_all:0.989\n","620/1500 loss:0.139 acc:0.995 acc_all:0.989\n","630/1500 loss:0.139 acc:0.995 acc_all:0.989\n","640/1500 loss:0.139 acc:0.995 acc_all:0.990\n","650/1500 loss:0.139 acc:0.995 acc_all:0.990\n","660/1500 loss:0.139 acc:0.995 acc_all:0.990\n","670/1500 loss:0.139 acc:0.995 acc_all:0.990\n","680/1500 loss:0.139 acc:0.995 acc_all:0.990\n","690/1500 loss:0.139 acc:0.995 acc_all:0.990\n","700/1500 loss:0.139 acc:0.995 acc_all:0.991\n","710/1500 loss:0.139 acc:0.995 acc_all:0.991\n","720/1500 loss:0.139 acc:0.995 acc_all:0.991\n","730/1500 loss:0.139 acc:0.995 acc_all:0.991\n","740/1500 loss:0.139 acc:0.996 acc_all:0.991\n","750/1500 loss:0.139 acc:0.996 acc_all:0.991\n","760/1500 loss:0.139 acc:0.996 acc_all:0.991\n","770/1500 loss:0.139 acc:0.996 acc_all:0.991\n","780/1500 loss:0.139 acc:0.996 acc_all:0.991\n","790/1500 loss:0.139 acc:0.996 acc_all:0.991\n","800/1500 loss:0.139 acc:0.996 acc_all:0.992\n","810/1500 loss:0.139 acc:0.996 acc_all:0.992\n","820/1500 loss:0.139 acc:0.996 acc_all:0.992\n","830/1500 loss:0.139 acc:0.996 acc_all:0.992\n","840/1500 loss:0.139 acc:0.996 acc_all:0.992\n","850/1500 loss:0.139 acc:0.996 acc_all:0.992\n","860/1500 loss:0.139 acc:0.996 acc_all:0.992\n","870/1500 loss:0.139 acc:0.996 acc_all:0.992\n","880/1500 loss:0.139 acc:0.996 acc_all:0.992\n","890/1500 loss:0.139 acc:0.996 acc_all:0.992\n","900/1500 loss:0.139 acc:0.996 acc_all:0.992\n","910/1500 loss:0.139 acc:0.996 acc_all:0.993\n","920/1500 loss:0.139 acc:0.996 acc_all:0.993\n","930/1500 loss:0.139 acc:0.996 acc_all:0.993\n","940/1500 loss:0.139 acc:0.996 acc_all:0.993\n","950/1500 loss:0.139 acc:0.996 acc_all:0.993\n","960/1500 loss:0.139 acc:0.996 acc_all:0.992\n","970/1500 loss:0.139 acc:0.996 acc_all:0.992\n","980/1500 loss:0.139 acc:0.996 acc_all:0.992\n","990/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1000/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1010/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1020/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1030/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1040/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1050/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1060/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1070/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1080/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1090/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1100/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1110/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1120/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1130/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1140/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1150/1500 loss:0.139 acc:0.996 acc_all:0.992\n","1160/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1170/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1180/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1190/1500 loss:0.139 acc:0.997 acc_all:0.993\n","1200/1500 loss:0.139 acc:0.997 acc_all:0.993\n","1210/1500 loss:0.139 acc:0.997 acc_all:0.993\n","1220/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1230/1500 loss:0.139 acc:0.997 acc_all:0.993\n","1240/1500 loss:0.139 acc:0.997 acc_all:0.993\n","1250/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1260/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1270/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1280/1500 loss:0.139 acc:0.996 acc_all:0.993\n","1290/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1300/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1310/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1320/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1330/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1340/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1350/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1360/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1370/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1380/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1390/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1400/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1410/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1420/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1430/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1440/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1450/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1460/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1470/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1480/1500 loss:0.138 acc:0.996 acc_all:0.992\n","1490/1500 loss:0.138 acc:0.996 acc_all:0.992\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wEV_J9hNfHLn","colab_type":"code","outputId":"8236e21f-a1fa-4704-bd05-216617ec3a14","executionInfo":{"status":"ok","timestamp":1580397501469,"user_tz":-540,"elapsed":1781,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":160}},"source":["# 테스트셋의 성능\n","h = model(x_test_set)\n","model.get_accuracy(y_test_set, h)\n","print('개별정확도',model.acc.numpy(),'두자리 모두 맞춘 정확도', model.acc_all.numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer my_model_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","개별정확도 0.95434296 두자리 모두 맞춘 정확도 0.9097996\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ZY6yU7QCX_q","colab_type":"code","colab":{}},"source":["class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n","  def __init__(self):    \n","    super(MyModel, self).__init__()\n","    self.k = 10+1 # 클래스 갯수 \n","    self.seq = 2 # 자릿수\n","    self.opt = tf.optimizers.Adam(learning_rate=0.001)#Stochatic Gradient Descent 확률적 경사 하강\n","    # self.conv0 = keras.layers.Conv2D(16, [3,3], padding='same', activation=keras.activations.relu)\n","    # self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same', activation=keras.activations.relu)\n","    self.conv0 = keras.layers.Conv2D(16, [3,3], padding='same')\n","    self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same')\n","    self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n","    self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')    \n","    self.rnn = keras.layers.LSTM(units=self.k, return_sequences=True)\n","    # self.rnn = keras.layers.GRU(units=self.k, return_sequences=True)\n","    self.batch = keras.layers.BatchNormalization(axis=1)\n","    self.act = keras.layers.ReLU()\n","  \n","  def call(self, x):\n","    #x (1797, 64)\n","    x_4d = tf.reshape(x, [-1,8,8*2,1]) \n","    x_4d = tf.cast(x_4d, tf.float32)\n","    net = self.conv0(x_4d)\n","    net = self.batch(net)\n","    net = self.act(net)\n","    net = self.pool0(net) #(4,8,16)\n","    net = self.conv1(net)\n","    net = self.batch(net)\n","    net = self.act(net)\n","    net = self.pool1(net) #(2,4,32)\n","    net = self.pool1(net) #(1,2,32)\n","    net = tf.squeeze(net, axis=1) #(2,32)\n","    h = self.rnn(net)#(2,10)        \n","    h = tf.nn.softmax(h, axis=2)\n","    return h\n","\n","  def get_loss(self, y, h):\n","    #학습할때 nan이 발생하는 경우 값을 clip(자르다) (최소값, 최대값) \n","    h = tf.clip_by_value(h, 1e-8, 1 - 1e-8) # h 가 0이나 1이 되지 않도록 하는 안전장치 \n","    cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n","    loss = tf.reduce_mean(cross_entropy)\n","    return loss\n","\n","  def get_accuracy(self, y, h):    \n","    predict = tf.argmax(h, -1)\n","    is_equal = tf.equal(y, predict)\n","    self.acc = tf.reduce_mean(tf.cast(is_equal, tf.float32)) # True > 1, False > 0 로 cast\n","    self.acc_all = tf.reduce_mean(tf.cast(tf.reduce_all(is_equal, axis=1), tf.float32))\n","\n","  def fit(self, x, y, epoch=1):\n","    # x : (m, 8, 16), y: (m, 2)    \n","    y_hot = tf.one_hot(y, depth=self.k, axis=-1)#(m, 2, 10)  \n","    for i in range(epoch):\n","      with tf.GradientTape() as tape: #경사 기록 장치\n","        h = self.call(x)\n","        loss = self.get_loss(y_hot, h)        \n","      grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n","      #경사가 너무 크면 nan 이 될 수 있으므로 gradient cliping (최소,최대값 제한) 을 합니다\n","      grads = [(tf.clip_by_value(grad, -5.0, 5.0)) for grad in grads]      \n","      self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n","      self.get_accuracy(y, h)\n","      if i%10==0:\n","        print('%d/%d loss:%.3f acc:%.3f acc_all:%.3f'%(i, epoch, loss, self.acc, self.acc_all))\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBefZrfYCvvJ","colab_type":"code","outputId":"f612105a-ed68-47ae-d29f-c6ccffd6a5aa","executionInfo":{"status":"ok","timestamp":1580397771250,"user_tz":-540,"elapsed":29320,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit(x_train_set, y_train_set, 1500)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0/1500 loss:0.306 acc:0.075 acc_all:0.016\n","10/1500 loss:0.288 acc:0.348 acc_all:0.078\n","20/1500 loss:0.267 acc:0.552 acc_all:0.278\n","30/1500 loss:0.248 acc:0.600 acc_all:0.348\n","40/1500 loss:0.237 acc:0.600 acc_all:0.357\n","50/1500 loss:0.229 acc:0.612 acc_all:0.372\n","60/1500 loss:0.224 acc:0.619 acc_all:0.381\n","70/1500 loss:0.221 acc:0.621 acc_all:0.383\n","80/1500 loss:0.218 acc:0.623 acc_all:0.386\n","90/1500 loss:0.215 acc:0.625 acc_all:0.388\n","100/1500 loss:0.212 acc:0.625 acc_all:0.389\n","110/1500 loss:0.210 acc:0.625 acc_all:0.388\n","120/1500 loss:0.208 acc:0.625 acc_all:0.388\n","130/1500 loss:0.207 acc:0.626 acc_all:0.389\n","140/1500 loss:0.206 acc:0.626 acc_all:0.390\n","150/1500 loss:0.206 acc:0.626 acc_all:0.390\n","160/1500 loss:0.205 acc:0.627 acc_all:0.391\n","170/1500 loss:0.205 acc:0.627 acc_all:0.392\n","180/1500 loss:0.205 acc:0.627 acc_all:0.392\n","190/1500 loss:0.204 acc:0.628 acc_all:0.392\n","200/1500 loss:0.204 acc:0.628 acc_all:0.392\n","210/1500 loss:0.204 acc:0.628 acc_all:0.393\n","220/1500 loss:0.204 acc:0.629 acc_all:0.394\n","230/1500 loss:0.203 acc:0.631 acc_all:0.395\n","240/1500 loss:0.203 acc:0.632 acc_all:0.397\n","250/1500 loss:0.203 acc:0.633 acc_all:0.399\n","260/1500 loss:0.203 acc:0.635 acc_all:0.401\n","270/1500 loss:0.203 acc:0.638 acc_all:0.404\n","280/1500 loss:0.203 acc:0.640 acc_all:0.406\n","290/1500 loss:0.203 acc:0.642 acc_all:0.408\n","300/1500 loss:0.202 acc:0.643 acc_all:0.409\n","310/1500 loss:0.202 acc:0.646 acc_all:0.412\n","320/1500 loss:0.202 acc:0.648 acc_all:0.415\n","330/1500 loss:0.202 acc:0.652 acc_all:0.417\n","340/1500 loss:0.202 acc:0.654 acc_all:0.420\n","350/1500 loss:0.202 acc:0.658 acc_all:0.424\n","360/1500 loss:0.202 acc:0.661 acc_all:0.428\n","370/1500 loss:0.202 acc:0.664 acc_all:0.431\n","380/1500 loss:0.202 acc:0.666 acc_all:0.433\n","390/1500 loss:0.202 acc:0.668 acc_all:0.435\n","400/1500 loss:0.202 acc:0.670 acc_all:0.437\n","410/1500 loss:0.202 acc:0.672 acc_all:0.441\n","420/1500 loss:0.202 acc:0.675 acc_all:0.443\n","430/1500 loss:0.202 acc:0.676 acc_all:0.445\n","440/1500 loss:0.201 acc:0.676 acc_all:0.445\n","450/1500 loss:0.201 acc:0.677 acc_all:0.447\n","460/1500 loss:0.201 acc:0.678 acc_all:0.448\n","470/1500 loss:0.201 acc:0.679 acc_all:0.448\n","480/1500 loss:0.201 acc:0.679 acc_all:0.450\n","490/1500 loss:0.201 acc:0.681 acc_all:0.452\n","500/1500 loss:0.201 acc:0.682 acc_all:0.454\n","510/1500 loss:0.201 acc:0.682 acc_all:0.454\n","520/1500 loss:0.201 acc:0.683 acc_all:0.455\n","530/1500 loss:0.201 acc:0.683 acc_all:0.455\n","540/1500 loss:0.201 acc:0.683 acc_all:0.455\n","550/1500 loss:0.201 acc:0.685 acc_all:0.457\n","560/1500 loss:0.201 acc:0.686 acc_all:0.459\n","570/1500 loss:0.201 acc:0.688 acc_all:0.462\n","580/1500 loss:0.201 acc:0.689 acc_all:0.464\n","590/1500 loss:0.201 acc:0.691 acc_all:0.465\n","600/1500 loss:0.201 acc:0.692 acc_all:0.467\n","610/1500 loss:0.201 acc:0.694 acc_all:0.470\n","620/1500 loss:0.201 acc:0.695 acc_all:0.471\n","630/1500 loss:0.201 acc:0.696 acc_all:0.472\n","640/1500 loss:0.201 acc:0.697 acc_all:0.474\n","650/1500 loss:0.201 acc:0.699 acc_all:0.476\n","660/1500 loss:0.201 acc:0.699 acc_all:0.477\n","670/1500 loss:0.201 acc:0.699 acc_all:0.477\n","680/1500 loss:0.201 acc:0.700 acc_all:0.477\n","690/1500 loss:0.201 acc:0.700 acc_all:0.477\n","700/1500 loss:0.201 acc:0.701 acc_all:0.479\n","710/1500 loss:0.201 acc:0.703 acc_all:0.482\n","720/1500 loss:0.201 acc:0.703 acc_all:0.483\n","730/1500 loss:0.201 acc:0.703 acc_all:0.483\n","740/1500 loss:0.201 acc:0.704 acc_all:0.485\n","750/1500 loss:0.201 acc:0.704 acc_all:0.485\n","760/1500 loss:0.201 acc:0.705 acc_all:0.486\n","770/1500 loss:0.201 acc:0.706 acc_all:0.487\n","780/1500 loss:0.201 acc:0.706 acc_all:0.487\n","790/1500 loss:0.201 acc:0.707 acc_all:0.487\n","800/1500 loss:0.201 acc:0.707 acc_all:0.489\n","810/1500 loss:0.201 acc:0.708 acc_all:0.490\n","820/1500 loss:0.201 acc:0.709 acc_all:0.491\n","830/1500 loss:0.201 acc:0.709 acc_all:0.492\n","840/1500 loss:0.201 acc:0.711 acc_all:0.493\n","850/1500 loss:0.201 acc:0.711 acc_all:0.494\n","860/1500 loss:0.201 acc:0.713 acc_all:0.496\n","870/1500 loss:0.201 acc:0.713 acc_all:0.496\n","880/1500 loss:0.201 acc:0.714 acc_all:0.498\n","890/1500 loss:0.201 acc:0.715 acc_all:0.499\n","900/1500 loss:0.201 acc:0.716 acc_all:0.501\n","910/1500 loss:0.201 acc:0.717 acc_all:0.502\n","920/1500 loss:0.201 acc:0.717 acc_all:0.502\n","930/1500 loss:0.201 acc:0.717 acc_all:0.503\n","940/1500 loss:0.201 acc:0.718 acc_all:0.505\n","950/1500 loss:0.201 acc:0.719 acc_all:0.505\n","960/1500 loss:0.201 acc:0.719 acc_all:0.506\n","970/1500 loss:0.201 acc:0.720 acc_all:0.507\n","980/1500 loss:0.201 acc:0.721 acc_all:0.508\n","990/1500 loss:0.201 acc:0.721 acc_all:0.509\n","1000/1500 loss:0.201 acc:0.721 acc_all:0.510\n","1010/1500 loss:0.201 acc:0.722 acc_all:0.510\n","1020/1500 loss:0.201 acc:0.722 acc_all:0.511\n","1030/1500 loss:0.201 acc:0.722 acc_all:0.512\n","1040/1500 loss:0.200 acc:0.723 acc_all:0.513\n","1050/1500 loss:0.200 acc:0.724 acc_all:0.513\n","1060/1500 loss:0.200 acc:0.724 acc_all:0.513\n","1070/1500 loss:0.200 acc:0.724 acc_all:0.513\n","1080/1500 loss:0.200 acc:0.724 acc_all:0.514\n","1090/1500 loss:0.200 acc:0.725 acc_all:0.514\n","1100/1500 loss:0.200 acc:0.725 acc_all:0.516\n","1110/1500 loss:0.200 acc:0.726 acc_all:0.517\n","1120/1500 loss:0.200 acc:0.726 acc_all:0.517\n","1130/1500 loss:0.200 acc:0.726 acc_all:0.517\n","1140/1500 loss:0.200 acc:0.726 acc_all:0.518\n","1150/1500 loss:0.200 acc:0.727 acc_all:0.518\n","1160/1500 loss:0.200 acc:0.727 acc_all:0.519\n","1170/1500 loss:0.200 acc:0.727 acc_all:0.519\n","1180/1500 loss:0.200 acc:0.727 acc_all:0.519\n","1190/1500 loss:0.200 acc:0.727 acc_all:0.519\n","1200/1500 loss:0.200 acc:0.728 acc_all:0.520\n","1210/1500 loss:0.200 acc:0.728 acc_all:0.520\n","1220/1500 loss:0.200 acc:0.729 acc_all:0.521\n","1230/1500 loss:0.200 acc:0.729 acc_all:0.522\n","1240/1500 loss:0.200 acc:0.729 acc_all:0.522\n","1250/1500 loss:0.200 acc:0.730 acc_all:0.524\n","1260/1500 loss:0.200 acc:0.731 acc_all:0.524\n","1270/1500 loss:0.200 acc:0.731 acc_all:0.525\n","1280/1500 loss:0.200 acc:0.731 acc_all:0.525\n","1290/1500 loss:0.200 acc:0.731 acc_all:0.525\n","1300/1500 loss:0.200 acc:0.731 acc_all:0.525\n","1310/1500 loss:0.200 acc:0.731 acc_all:0.525\n","1320/1500 loss:0.200 acc:0.731 acc_all:0.525\n","1330/1500 loss:0.200 acc:0.731 acc_all:0.526\n","1340/1500 loss:0.200 acc:0.731 acc_all:0.527\n","1350/1500 loss:0.200 acc:0.732 acc_all:0.527\n","1360/1500 loss:0.200 acc:0.732 acc_all:0.528\n","1370/1500 loss:0.200 acc:0.732 acc_all:0.528\n","1380/1500 loss:0.200 acc:0.733 acc_all:0.529\n","1390/1500 loss:0.200 acc:0.733 acc_all:0.530\n","1400/1500 loss:0.200 acc:0.733 acc_all:0.530\n","1410/1500 loss:0.200 acc:0.734 acc_all:0.530\n","1420/1500 loss:0.200 acc:0.734 acc_all:0.531\n","1430/1500 loss:0.200 acc:0.734 acc_all:0.532\n","1440/1500 loss:0.200 acc:0.735 acc_all:0.533\n","1450/1500 loss:0.200 acc:0.736 acc_all:0.534\n","1460/1500 loss:0.200 acc:0.736 acc_all:0.534\n","1470/1500 loss:0.200 acc:0.736 acc_all:0.535\n","1480/1500 loss:0.200 acc:0.737 acc_all:0.536\n","1490/1500 loss:0.200 acc:0.737 acc_all:0.536\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x_Nn2qg7Cxnl","colab_type":"code","outputId":"b3c27af8-fdfa-4963-cc39-4f427e6eefb8","executionInfo":{"status":"ok","timestamp":1580397799783,"user_tz":-540,"elapsed":1205,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":160}},"source":["# 테스트셋의 성능\n","h = model(x_test_set)\n","model.get_accuracy(y_test_set, h)\n","print('개별정확도',model.acc.numpy(),'두자리 모두 맞춘 정확도', model.acc_all.numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer my_model_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","개별정확도 0.71436524 두자리 모두 맞춘 정확도 0.49777284\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0Pyg5hz5ilz5","colab_type":"code","colab":{}},"source":["class MyModel(keras.Model): # github.com/sogangori/choongang20/ \n","  def __init__(self):    \n","    super(MyModel, self).__init__()\n","    self.k = 10+1 # 클래스 갯수 \n","    self.seq = 2 # 자릿수\n","    self.opt = tf.optimizers.Nadam(learning_rate=0.0001)#Stochatic Gradient Descent 확률적 경사 하강\n","    self.conv0 = keras.layers.Conv2D(64, [3,3], padding='same')\n","    self.conv1 = keras.layers.Conv2D(32, [3,3], padding='same')\n","    self.pool0 = keras.layers.MaxPool2D([2,2], padding='same')\n","    self.pool1 = keras.layers.MaxPool2D([2,2], padding='same')    \n","    # self.rnn = keras.layers.LSTM(units=self.k, return_sequences=True)\n","    self.rnn = keras.layers.GRU(units=self.k, return_sequences=True)\n","    self.batch = keras.layers.BatchNormalization(axis=1)\n","    self.act = keras.layers.ReLU()\n","  \n","  def call(self, x):\n","    #x (1797, 64)\n","    x_4d = tf.reshape(x, [-1,8,8*2,1]) \n","    x_4d = tf.cast(x_4d, tf.float32)\n","    net = self.conv0(x_4d)\n","    net = self.batch(net)\n","    net = self.act(net)\n","    net = self.pool0(net)#(4,8,16)\n","    net = self.conv1(net)\n","    net = self.batch(net)\n","    net = self.act(net)\n","    net = self.pool1(net)#(2,4,32)\n","    net = tf.reduce_sum(net, axis=1) # 4개의 시퀀스\n","    h = self.rnn(net) #(4,11) 4개의 output        \n","    h = h[:, 2:] # 앞에 2개의 output은 버리고 뒤의 2개 output 사용\n","    h = tf.nn.softmax(h, axis=2)\n","    return h\n","\n","  def get_loss(self, y, h):\n","    #학습할때 nan이 발생하는 경우 값을 clip(자르다) (최소값, 최대값) \n","    h = tf.clip_by_value(h, 1e-8, 1 - 1e-8) # h 가 0이나 1이 되지 않도록 하는 안전장치 \n","    cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n","    loss = tf.reduce_mean(cross_entropy)\n","    return loss\n","\n","  def get_accuracy(self, y, h):    \n","    predict = tf.argmax(h, -1)\n","    is_equal = tf.equal(y, predict)\n","    self.acc = tf.reduce_mean(tf.cast(is_equal, tf.float32)) # True > 1, False > 0 로 cast\n","    self.acc_all = tf.reduce_mean(tf.cast(tf.reduce_all(is_equal, axis=1), tf.float32))\n","\n","  def fit(self, x, y, epoch=1):\n","    # x : (m, 8, 16), y: (m, 2)    \n","    y_hot = tf.one_hot(y, depth=self.k, axis=-1)#(m, 2, 10)  \n","    for i in range(epoch):\n","      with tf.GradientTape() as tape: #경사 기록 장치\n","        h = self.call(x)\n","        loss = self.get_loss(y_hot, h)        \n","      grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n","      #경사가 너무 크면 nan 이 될 수 있으므로 gradient cliping (최소,최대값 제한) 을 합니다\n","      grads = [(tf.clip_by_value(grad, -5.0, 5.0)) for grad in grads]      \n","      self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n","      self.get_accuracy(y, h)\n","      if i%10==0:\n","        print('%d/%d loss:%.3f acc:%.3f acc_all:%.3f'%(i, epoch, loss, self.acc, self.acc_all))\n","model = MyModel()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S8OWeXHjSHw","colab_type":"code","outputId":"664e0765-a0ef-4454-d6cc-23b6f294026e","executionInfo":{"status":"error","timestamp":1580397024365,"user_tz":-540,"elapsed":394071,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model.fit(x_train_shuffle, y_train_shuffle, 5000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0/5000 loss:0.315 acc:0.032 acc_all:0.002\n","10/5000 loss:0.307 acc:0.077 acc_all:0.008\n","20/5000 loss:0.300 acc:0.096 acc_all:0.018\n","30/5000 loss:0.296 acc:0.116 acc_all:0.024\n","40/5000 loss:0.292 acc:0.150 acc_all:0.037\n","50/5000 loss:0.288 acc:0.190 acc_all:0.053\n","60/5000 loss:0.284 acc:0.254 acc_all:0.079\n","70/5000 loss:0.280 acc:0.334 acc_all:0.101\n","80/5000 loss:0.276 acc:0.434 acc_all:0.140\n","90/5000 loss:0.272 acc:0.502 acc_all:0.182\n","100/5000 loss:0.268 acc:0.535 acc_all:0.205\n","110/5000 loss:0.264 acc:0.555 acc_all:0.227\n","120/5000 loss:0.261 acc:0.570 acc_all:0.245\n","130/5000 loss:0.257 acc:0.575 acc_all:0.248\n","140/5000 loss:0.254 acc:0.578 acc_all:0.249\n","150/5000 loss:0.250 acc:0.587 acc_all:0.260\n","160/5000 loss:0.247 acc:0.595 acc_all:0.271\n","170/5000 loss:0.245 acc:0.603 acc_all:0.283\n","180/5000 loss:0.242 acc:0.606 acc_all:0.286\n","190/5000 loss:0.239 acc:0.606 acc_all:0.285\n","200/5000 loss:0.237 acc:0.612 acc_all:0.294\n","210/5000 loss:0.234 acc:0.617 acc_all:0.304\n","220/5000 loss:0.232 acc:0.621 acc_all:0.312\n","230/5000 loss:0.230 acc:0.624 acc_all:0.317\n","240/5000 loss:0.228 acc:0.627 acc_all:0.321\n","250/5000 loss:0.226 acc:0.629 acc_all:0.324\n","260/5000 loss:0.224 acc:0.635 acc_all:0.331\n","270/5000 loss:0.222 acc:0.644 acc_all:0.341\n","280/5000 loss:0.220 acc:0.653 acc_all:0.351\n","290/5000 loss:0.219 acc:0.660 acc_all:0.361\n","300/5000 loss:0.217 acc:0.667 acc_all:0.369\n","310/5000 loss:0.216 acc:0.668 acc_all:0.373\n","320/5000 loss:0.214 acc:0.672 acc_all:0.380\n","330/5000 loss:0.213 acc:0.676 acc_all:0.386\n","340/5000 loss:0.211 acc:0.680 acc_all:0.393\n","350/5000 loss:0.210 acc:0.686 acc_all:0.404\n","360/5000 loss:0.209 acc:0.690 acc_all:0.412\n","370/5000 loss:0.207 acc:0.697 acc_all:0.422\n","380/5000 loss:0.206 acc:0.703 acc_all:0.434\n","390/5000 loss:0.205 acc:0.714 acc_all:0.452\n","400/5000 loss:0.203 acc:0.722 acc_all:0.469\n","410/5000 loss:0.202 acc:0.731 acc_all:0.486\n","420/5000 loss:0.201 acc:0.739 acc_all:0.500\n","430/5000 loss:0.199 acc:0.747 acc_all:0.515\n","440/5000 loss:0.198 acc:0.758 acc_all:0.535\n","450/5000 loss:0.196 acc:0.769 acc_all:0.558\n","460/5000 loss:0.195 acc:0.779 acc_all:0.577\n","470/5000 loss:0.194 acc:0.786 acc_all:0.590\n","480/5000 loss:0.193 acc:0.793 acc_all:0.605\n","490/5000 loss:0.192 acc:0.800 acc_all:0.618\n","500/5000 loss:0.191 acc:0.809 acc_all:0.635\n","510/5000 loss:0.190 acc:0.821 acc_all:0.658\n","520/5000 loss:0.189 acc:0.833 acc_all:0.682\n","530/5000 loss:0.188 acc:0.845 acc_all:0.704\n","540/5000 loss:0.187 acc:0.852 acc_all:0.718\n","550/5000 loss:0.186 acc:0.859 acc_all:0.732\n","560/5000 loss:0.185 acc:0.864 acc_all:0.741\n","570/5000 loss:0.184 acc:0.868 acc_all:0.749\n","580/5000 loss:0.183 acc:0.870 acc_all:0.754\n","590/5000 loss:0.182 acc:0.873 acc_all:0.761\n","600/5000 loss:0.181 acc:0.877 acc_all:0.768\n","610/5000 loss:0.180 acc:0.880 acc_all:0.774\n","620/5000 loss:0.179 acc:0.882 acc_all:0.778\n","630/5000 loss:0.178 acc:0.885 acc_all:0.783\n","640/5000 loss:0.178 acc:0.887 acc_all:0.788\n","650/5000 loss:0.177 acc:0.889 acc_all:0.791\n","660/5000 loss:0.176 acc:0.891 acc_all:0.796\n","670/5000 loss:0.175 acc:0.893 acc_all:0.800\n","680/5000 loss:0.175 acc:0.896 acc_all:0.805\n","690/5000 loss:0.174 acc:0.898 acc_all:0.809\n","700/5000 loss:0.174 acc:0.899 acc_all:0.812\n","710/5000 loss:0.173 acc:0.903 acc_all:0.818\n","720/5000 loss:0.172 acc:0.904 acc_all:0.820\n","730/5000 loss:0.172 acc:0.908 acc_all:0.826\n","740/5000 loss:0.171 acc:0.912 acc_all:0.831\n","750/5000 loss:0.171 acc:0.917 acc_all:0.840\n","760/5000 loss:0.170 acc:0.923 acc_all:0.851\n","770/5000 loss:0.170 acc:0.928 acc_all:0.859\n","780/5000 loss:0.169 acc:0.933 acc_all:0.868\n","790/5000 loss:0.169 acc:0.934 acc_all:0.870\n","800/5000 loss:0.168 acc:0.935 acc_all:0.872\n","810/5000 loss:0.167 acc:0.936 acc_all:0.874\n","820/5000 loss:0.167 acc:0.936 acc_all:0.874\n","830/5000 loss:0.166 acc:0.937 acc_all:0.877\n","840/5000 loss:0.165 acc:0.938 acc_all:0.878\n","850/5000 loss:0.165 acc:0.939 acc_all:0.881\n","860/5000 loss:0.165 acc:0.940 acc_all:0.883\n","870/5000 loss:0.164 acc:0.942 acc_all:0.886\n","880/5000 loss:0.164 acc:0.943 acc_all:0.888\n","890/5000 loss:0.164 acc:0.944 acc_all:0.890\n","900/5000 loss:0.163 acc:0.944 acc_all:0.891\n","910/5000 loss:0.163 acc:0.944 acc_all:0.890\n","920/5000 loss:0.162 acc:0.944 acc_all:0.891\n","930/5000 loss:0.162 acc:0.946 acc_all:0.893\n","940/5000 loss:0.162 acc:0.947 acc_all:0.896\n","950/5000 loss:0.161 acc:0.948 acc_all:0.897\n","960/5000 loss:0.161 acc:0.949 acc_all:0.899\n","970/5000 loss:0.161 acc:0.950 acc_all:0.901\n","980/5000 loss:0.160 acc:0.950 acc_all:0.902\n","990/5000 loss:0.160 acc:0.951 acc_all:0.903\n","1000/5000 loss:0.160 acc:0.951 acc_all:0.905\n","1010/5000 loss:0.160 acc:0.953 acc_all:0.907\n","1020/5000 loss:0.160 acc:0.953 acc_all:0.907\n","1030/5000 loss:0.159 acc:0.953 acc_all:0.907\n","1040/5000 loss:0.159 acc:0.953 acc_all:0.908\n","1050/5000 loss:0.159 acc:0.953 acc_all:0.908\n","1060/5000 loss:0.159 acc:0.953 acc_all:0.909\n","1070/5000 loss:0.158 acc:0.954 acc_all:0.909\n","1080/5000 loss:0.158 acc:0.954 acc_all:0.909\n","1090/5000 loss:0.158 acc:0.954 acc_all:0.910\n","1100/5000 loss:0.158 acc:0.954 acc_all:0.911\n","1110/5000 loss:0.158 acc:0.955 acc_all:0.911\n","1120/5000 loss:0.158 acc:0.955 acc_all:0.912\n","1130/5000 loss:0.157 acc:0.955 acc_all:0.913\n","1140/5000 loss:0.157 acc:0.955 acc_all:0.913\n","1150/5000 loss:0.157 acc:0.956 acc_all:0.913\n","1160/5000 loss:0.157 acc:0.956 acc_all:0.913\n","1170/5000 loss:0.157 acc:0.956 acc_all:0.913\n","1180/5000 loss:0.157 acc:0.956 acc_all:0.914\n","1190/5000 loss:0.157 acc:0.957 acc_all:0.915\n","1200/5000 loss:0.156 acc:0.957 acc_all:0.915\n","1210/5000 loss:0.156 acc:0.957 acc_all:0.915\n","1220/5000 loss:0.156 acc:0.958 acc_all:0.916\n","1230/5000 loss:0.156 acc:0.958 acc_all:0.916\n","1240/5000 loss:0.156 acc:0.958 acc_all:0.916\n","1250/5000 loss:0.156 acc:0.958 acc_all:0.917\n","1260/5000 loss:0.156 acc:0.958 acc_all:0.916\n","1270/5000 loss:0.156 acc:0.958 acc_all:0.916\n","1280/5000 loss:0.156 acc:0.958 acc_all:0.917\n","1290/5000 loss:0.155 acc:0.958 acc_all:0.917\n","1300/5000 loss:0.155 acc:0.958 acc_all:0.917\n","1310/5000 loss:0.155 acc:0.959 acc_all:0.918\n","1320/5000 loss:0.155 acc:0.959 acc_all:0.918\n","1330/5000 loss:0.155 acc:0.959 acc_all:0.918\n","1340/5000 loss:0.155 acc:0.959 acc_all:0.918\n","1350/5000 loss:0.155 acc:0.959 acc_all:0.918\n","1360/5000 loss:0.155 acc:0.959 acc_all:0.918\n","1370/5000 loss:0.155 acc:0.959 acc_all:0.919\n","1380/5000 loss:0.155 acc:0.959 acc_all:0.919\n","1390/5000 loss:0.155 acc:0.959 acc_all:0.919\n","1400/5000 loss:0.155 acc:0.960 acc_all:0.921\n","1410/5000 loss:0.154 acc:0.960 acc_all:0.921\n","1420/5000 loss:0.154 acc:0.960 acc_all:0.921\n","1430/5000 loss:0.154 acc:0.960 acc_all:0.921\n","1440/5000 loss:0.154 acc:0.960 acc_all:0.921\n","1450/5000 loss:0.154 acc:0.960 acc_all:0.921\n","1460/5000 loss:0.154 acc:0.960 acc_all:0.921\n","1470/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1480/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1490/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1500/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1510/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1520/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1530/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1540/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1550/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1560/5000 loss:0.154 acc:0.961 acc_all:0.922\n","1570/5000 loss:0.154 acc:0.960 acc_all:0.922\n","1580/5000 loss:0.153 acc:0.960 acc_all:0.922\n","1590/5000 loss:0.153 acc:0.960 acc_all:0.922\n","1600/5000 loss:0.153 acc:0.960 acc_all:0.922\n","1610/5000 loss:0.153 acc:0.960 acc_all:0.922\n","1620/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1630/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1640/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1650/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1660/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1670/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1680/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1690/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1700/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1710/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1720/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1730/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1740/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1750/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1760/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1770/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1780/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1790/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1800/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1810/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1820/5000 loss:0.153 acc:0.961 acc_all:0.923\n","1830/5000 loss:0.152 acc:0.961 acc_all:0.923\n","1840/5000 loss:0.152 acc:0.961 acc_all:0.923\n","1850/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1860/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1870/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1880/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1890/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1900/5000 loss:0.152 acc:0.962 acc_all:0.925\n","1910/5000 loss:0.152 acc:0.962 acc_all:0.925\n","1920/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1930/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1940/5000 loss:0.152 acc:0.961 acc_all:0.924\n","1950/5000 loss:0.152 acc:0.962 acc_all:0.924\n","1960/5000 loss:0.152 acc:0.962 acc_all:0.925\n","1970/5000 loss:0.152 acc:0.962 acc_all:0.925\n","1980/5000 loss:0.152 acc:0.962 acc_all:0.925\n","1990/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2000/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2010/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2020/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2030/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2040/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2050/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2060/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2070/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2080/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2090/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2100/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2110/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2120/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2130/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2140/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2150/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2160/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2170/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2180/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2190/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2200/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2210/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2220/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2230/5000 loss:0.152 acc:0.962 acc_all:0.926\n","2240/5000 loss:0.152 acc:0.962 acc_all:0.926\n","2250/5000 loss:0.152 acc:0.963 acc_all:0.926\n","2260/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2270/5000 loss:0.152 acc:0.962 acc_all:0.925\n","2280/5000 loss:0.151 acc:0.962 acc_all:0.926\n","2290/5000 loss:0.151 acc:0.962 acc_all:0.926\n","2300/5000 loss:0.151 acc:0.962 acc_all:0.926\n","2310/5000 loss:0.151 acc:0.963 acc_all:0.926\n","2320/5000 loss:0.151 acc:0.963 acc_all:0.926\n","2330/5000 loss:0.151 acc:0.962 acc_all:0.926\n","2340/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2350/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2360/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2370/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2380/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2390/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2400/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2410/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2420/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2430/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2440/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2450/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2460/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2470/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2480/5000 loss:0.151 acc:0.963 acc_all:0.927\n","2490/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2500/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2510/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2520/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2530/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2540/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2550/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2560/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2570/5000 loss:0.151 acc:0.964 acc_all:0.928\n","2580/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2590/5000 loss:0.151 acc:0.964 acc_all:0.928\n","2600/5000 loss:0.151 acc:0.964 acc_all:0.928\n","2610/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2620/5000 loss:0.151 acc:0.964 acc_all:0.928\n","2630/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2640/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2650/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2660/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2670/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2680/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2690/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2700/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2710/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2720/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2730/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2740/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2750/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2760/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2770/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2780/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2790/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2800/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2810/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2820/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2830/5000 loss:0.151 acc:0.964 acc_all:0.930\n","2840/5000 loss:0.151 acc:0.964 acc_all:0.930\n","2850/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2860/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2870/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2880/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2890/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2900/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2910/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2920/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2930/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2940/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2950/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2960/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2970/5000 loss:0.151 acc:0.964 acc_all:0.929\n","2980/5000 loss:0.151 acc:0.963 acc_all:0.928\n","2990/5000 loss:0.151 acc:0.963 acc_all:0.928\n","3000/5000 loss:0.151 acc:0.963 acc_all:0.928\n","3010/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3020/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3030/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3040/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3050/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3060/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3070/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3080/5000 loss:0.151 acc:0.964 acc_all:0.929\n","3090/5000 loss:0.151 acc:0.964 acc_all:0.929\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7430\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Reshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7431\u001b[0;31m         tld.op_callbacks, tensor, shape)\n\u001b[0m\u001b[1;32m   7432\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-6d328aad4361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-91f09c7cda24>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#경사 기록 장치\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#경사 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-91f09c7cda24>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#x (1797, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mx_4d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mx_4d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_4d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_4d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7434\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7435\u001b[0m         return reshape_eager_fallback(\n\u001b[0;32m-> 7436\u001b[0;31m             tensor, shape, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   7437\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7438\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[0;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[1;32m   7456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7457\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreshape_eager_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7458\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7459\u001b[0m   \u001b[0m_attr_Tshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7460\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, default_dtype)\u001b[0m\n\u001b[1;32m    261\u001b[0m       ret.append(\n\u001b[1;32m    262\u001b[0m           ops.convert_to_tensor(\n\u001b[0;32m--> 263\u001b[0;31m               t, dtype, preferred_dtype=default_dtype, ctx=ctx))\n\u001b[0m\u001b[1;32m    264\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    315\u001b[0m                                          as_ref=False):\n\u001b[1;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 258\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"NPBvgG-IsgQj","colab_type":"code","outputId":"43da604c-7492-415e-9293-cb41d8d0b479","executionInfo":{"status":"ok","timestamp":1580397028779,"user_tz":-540,"elapsed":1231,"user":{"displayName":"김소영","photoUrl":"","userId":"07209527371816863636"}},"colab":{"base_uri":"https://localhost:8080/","height":160}},"source":["# 테스트셋의 성능\n","h = model(x_test_set)\n","model.get_accuracy(y_test_set, h)\n","print('개별정확도',model.acc.numpy(),'두자리 모두 맞춘 정확도', model.acc_all.numpy())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer my_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","개별정확도 0.88641423 두자리 모두 맞춘 정확도 0.7806236\n"],"name":"stdout"}]}]}